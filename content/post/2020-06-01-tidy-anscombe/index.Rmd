---
title: 'Tidyverse Case Study: Anscombe’s quartet'
date: '2020-06-01'
slug: tidy-anscombe
categories:
  - rstats
  - rbloggers
tags: 
  - tidyverse
  - anscombe
  - rstats
output: hugodown::hugo_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  comment = "#>", 
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  fig.retina = 2,
  out.width = "700px"
)
```

Anscombe's quartet is a really cool dataset that is used to illustrate
the importance of data visualisation. It even comes built into R (how cool is that?), and reading the helpfile, it states:

> [Anscombe's quartet] Four x-y datasets which have the same traditional statistical properties (mean, variance, correlation, regression line, etc.), yet are quite different.

Different, how?

The helpfile provides code exploring and visualising anscombe's quartet, presented below:

```{r anscombe-base}
require(stats); require(graphics)
summary(anscombe)

##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  print(anova(lmi))
}

## See how close they are (numerically!)
sapply(mods, coef)
lapply(mods, function(fm) coef(summary(fm)))

## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)

```

It's nice to see some fun style in the comments!

# Exploring Anscombe using the tidyverse

The helpfile does a great job of providing summaries of the data and plots!

There have been some recent changes with dplyr 1.0.0 coming out just the other day. I think it would be interesting to try doing the same steps as in the helpfile, but using the [tidyverse](https://tidyverse.org/) tools.

There are a few key parts to this analysis:

1. Tidy up the data
1. Explore the summary statistics of each group
1. Fit a model to each group
1. Make the plots in ggplot2.

## Tidy up the data

Before we tidy up the data, it is good to think about what format we want the data in.

Currently, the data is in this format:

```{r top-anscombe}
head(anscombe)
```

So how do we know the "right" format? 

One trick that I use is to imagine how I would write the ggplot code. Because this then allows me to work out what I want the columns to be.

So, perhaps at a basic level, I'd want code like this:

```{r gg-imagine, eval = FALSE}
ggplot(anscombe,
       aes(x = x,
           y = y)) + 
  geom_point() + 
  facet_wrap(~set)
```

So this tells me:

* The x axis maps onto the x column
* The y axis maps onto the y column
* I have a column, set

What we want is a format where we have:

```{r print-example, echo = FALSE}
short_df <- tibble::tribble(
  ~set,  ~x, ~y,
    1 ,  10, 8.04
)

knitr::kable(short_df)
```


In fact, reading the recent documentation for `tidyr`'s `pivot_longer` (the new version of `gather`), we have new example that shows you how to clean the `anscombe` data:

```{r anscombe-tidy}
library(tidyverse)

tidy_anscombe <- anscombe %>%
 pivot_longer(cols = everything(),
              names_to = c(".value", "set"),
              names_pattern = "(.)(.)")

tidy_anscombe
    ```

So, what happened there? Let's break it down.
  
```r
cols = everything()
```
>We pivot the data into a longer format, selecting _every column_ with:


```r
names_to = c(".value", "set")
```

> We specify how we want to create the new names of the data with:
  
Now this includes a nice bit of magic here, the ".value" command indicates a component of the name is also in the value. What does that mean? Well we go from:

```{r kable-anscombe, echo = FALSE}
library(knitr)
kable(anscombe[1:2, c(1,2,5,6)])
```

to

```{r kable-tidy-anscombe, echo = FALSE}
tidy_anscombe %>% 
  filter(set %in% c(1,2)) %>% 
  arrange(set) %>% 
  group_by(set) %>% 
  slice(1:2) %>% 
  kable()
```


Because the values `x1` and `y1`, the "names" that we want to create, are inherently tied to the names we want to get, "x" and "y". We are actually doing two steps here:

1. Splitting up the variables "x1" into "x" and 
2. A value "1" in the variable in "set". 

We need to split up the variable, and we need a way to specify how to do that:


```r
names_pattern = "(.)(.)"
```

> We describe the pattern that separates the values - two characters

I don't speak regex good, so I looked it up on ["regexr"](https://regexr.com/), and basically this translates to "two characters", so it will create a column for each character. You could equivalently write:

```r
names_pattern = "([a-z])([1-9])"
```

Which would more explicitly say: "The first thing is a character, the second thing is a number.

This means that we end up with:

```{r print-tidy-anscombe}
tidy_anscombe
```

Also just to give you a sense of the improvement of `pivot_longer` / `pivot_wider`, this is how I previously wrote this example (this blog post was started on 2017-12-08 and published on 2020-06-01) - and it didn't actually quite work and I gave up.

```{r old-anscombe}

old_tidy_anscombe <- anscombe %>%
  gather(key = "variable",
         value = "value") %>%
  as_tibble() %>%
  # So now we need to split this out into groups based on the number.
  mutate(group = readr::parse_number(variable),
         variable = stringr::str_remove(variable, "[1-9]")) %>%
  select(variable,
         group,
         value) %>%
  # And then we actually want to spread out the variable column
  rowid_to_column() %>%
  spread(key = variable,
         value = value)

old_tidy_anscombe
```

# Explore the summary statistics of each group

So we want to be able to do:

```{r summary-anscombe}
summary(anscombe)
```

But in a tidyverse way?

We can take this opportunity to use some of the new features in `dplyr` 1.0.0, 
`across` and co, combined with `tibble::lst()` to give us nice column names:

```{r anscombe-explore}
library(dplyr)

tidy_anscombe_summary <- tidy_anscombe %>%
  group_by(set) %>%
  summarise(across(.cols = everything(),
                   .fns = lst(min,max,median,mean,sd,var),
                   .names = "{col}_{fn}"))

tidy_anscombe_summary
```

Let's break that down.

```r
tidy_anscombe %>%
  group_by(set)
```

> For each set


```r
summarise(across(.cols = everything(),
```

> Calculate a one row summary (per group), across every variable:

```r
.fns = lst(min,max,median,mean,sd,var),
```

> Apply these functions 

(`lst` automatically names the functions after their input)

```{r list-names}
names(tibble::lst(max))
```


```r
.names = "{col}_{fn}"
```

> Then produce column names that inherit the column name, then the function name (thanks to our friend `lst`).

One more time, this is what we get:

```{r tidy-anscombe-summary}
tidy_anscombe_summary
```


There's no getting around the fact that `summary(anscombe)` is less work, in this instance. Like, it is actually amazing.

But, what you trade off there is the fact that you need data in an inherently inconvenient format for other parts of data analysis, and gain tools that allow you to express yourselve in a variety of circumstance well beyond `summary` can do. 

I think this is pretty dang neat!

## Fit a model to each group

This is a good one. 

```{r anscombe-model}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  print(anova(lmi))
}

## See how close they are (numerically!)
sapply(mods, coef)
lapply(mods, function(fm) coef(summary(fm)))


```

How to do this in tidyverse?

```{r broom-broom}
library(broom)

tidy_anscombe_models <- tidy_anscombe %>% 
  group_nest(set) %>% 
  mutate(fit = map(data, ~lm(y ~ x, data = .x)),
         tidy = map(fit, tidy),
         glance = map(fit, glance))
```

What this is doing is fitting a linear model to each of the groups in `set`, and then calculating summaries on the statistical model. Briefly:

```r
group_nest(set)
```

> Nest the data into groups based on set, which we can then do cool things with:

```r
fit = map(data, ~lm(y ~ x, data = .x))
```

> Fit a linear model to each of the datasets in `set`

```r
tidy = map(fit, tidy)
```

> Tidy up each of the linear models in `set` to get the coefficient information

```r
glance = map(fit, glance))
```

> Get the model fit statistics for each of the models.


Now, we unnest the `tidy_anscombe_models`:

```{r unnest-tidy}
tidy_anscombe_models %>% 
  unnest(cols = c(tidy)) 
```

But we'll need to make the dataset wider with `pivot_wider`:

```{r unnest-coeffs}
tidy_anscombe_models %>% 
  unnest(cols = c(tidy)) %>% 
  pivot_wider(id_cols = c(set,term),
              names_from = term,
              values_from = estimate)
```

Wow, so...the linear models give us basically the same information? Jeez.

> But what do these `pivot_wider` things do?

I hear you cry?

Well, briefly:

```r
id_cols = c(set,term)
```

> Which columns give us unique rows that you want to make wider?

```r
names_from = term
```

> Which column contains the information we want to take names from?

```r
values_from = estimate
```

> Which column contains information we want to take the values from?

OK so maybe the model coefficients are bad, but surely some of the other model fit statistics help identify differences between the sets? Right? 

```{r glance-anscombe}
tidy_anscombe_models %>% 
  unnest(cols = c(glance)) %>% 
  select(set, r.squared:df.residual)
```

Oh, dear. They're all the same.

# Make the plots in ggplot (which you should have done in the first place).

```{r anscombe-plot}
library(ggplot2)
ggplot(tidy_anscombe,
       aes(x = x,
           y = y)) +
  geom_point() + 
  facet_wrap(~set) +
  geom_smooth(method = "lm", se = FALSE)

```

Yeah wow, the data really is quite different. Should have done that first!

# Some thoughts

In writing this I noticed that, well, this is actually pretty dang hard. It covers a large chunk of skills you need to do a data analysis:

1. Tidy up data
1. Explore summary statistics
1. Fit a (or many!) model(s)
1. Make some plots.

It certainly packs a punch, for such a small dataset.

## The state of the tools

I think it's easy to look at the base R code in the helpfile for `?anscombe` and say "oh, there's better ways to write that code now!". 

Well, sure, there might be. But I can't help but think there are _always_ going to be better ways to write the code. I'm certain that there are better ways to write the code I created for this blog post.

But that's kind of not the point, the point is to demonstrate how to explore a dataset. The authors of the helpfile provided that for us. And if I wanted to explore the data myself without that helpfile, it would have actually been harder.

Let's reflect on this for a moment. R include the `anscombe` dataset **in it's base (well, datasets) distribution**, and not only that, **in the help file, provide code that replicates the summary statistics and graphics presented by Anscombe in 1973**. 

We absolutely get the point of the summaries and the graphics.

And I think that is something to celebrate, I'm not sure who wrote that help file, but I think that was very kind of them.

# Further reading

Check out [Rasmus Bååth's post on fitting a Bayesian spike-slab model to 
accurately fit to anscombe's quartet](http://www.sumsar.net/blog/2013/06/bayesian-modeling-of-anscombes-quartet/). Cool stuff!

[David Robinsons's RPubs on tidyverse plotting of Anscombe's quartet](https://rpubs.com/dgrtwo/tidy-anscombe). I wish I read this back in 2017.

[Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats)

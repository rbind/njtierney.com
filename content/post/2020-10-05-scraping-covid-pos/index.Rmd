---
title: Fancy Times and Scales with COVID data
author: Nicholas Tierney
date: '2020-10-11'
slug: times-scales-covid
categories:
  - rstats
  - scales
  - time series
  - covid19
  - data visualisation
tags:
  - rstats
  - scales
  - time series
  - covid19
  - data visualisation
output: hugodown::md_document
---

```{r setup, include = FALSE}
options(cli.width = 70)  # For tidyverse loading messages
knitr::opts_chunk$set(
  tidy.opts = list(width.cutoff = 70),  # For code
  width = 70,
  collapse = TRUE, 
  comment = "#>", 
  fig.width = 7, 
  fig.align = 'center',
  fig.asp = 0.618, # 1 / phi
  fig.retina = 2,
  out.width = "700px"
)
```

Ah, COVID19. Yet another COVID19 blog post? Kinda? Not really? This blog post covers how to:

- Scrape nicely formatted tables from a website with [`polite`](https://dmi3kno.github.io/polite/), [`rvest`](https://rvest.tidyverse.org/) and the [`tidyverse`](https://tidyverse.org/)
- Format dates with `strptime`
- Filter out dates using [`tsibble`](https://tsibble.tidyverts.org/)
- Use nicely formatted percentages in [`ggplot2`](https://ggplot2.tidyverse.org/) with [`scales`](https://scales.r-lib.org/).

We're in lockdown here in Melbourne and I find myself looking at all the case numbers every day. A number that I've been paying attention to helps is the positive test rate - the number of positive tests divided by the number of total tests. 

There's a great website, [covidlive.com.au](https://covidlive.com.au/), posted on [covidliveau](https://twitter.com/covidliveau), maintained by [Anthony Macali ](https://twitter.com/migga)

We're going to look at the [daily positive test rates for Victoria](https://covidlive.com.au/report/daily-positive-test-rate/vic), first let's load up the three packages we'll need, the `tidyverse` for general data manipulation and plotting and friends, `rvest` for web scraping, and `polite` for ethically managing the webscraping.

```{r libraries}
library(tidyverse)
library(rvest)
library(polite)
conflicted::conflict_prefer("pluck", "purrr")
conflicted::conflict_prefer("filter", "dplyr")
```

(Note that I'm saying to prefer `pluck` from `purrr`, since there is a namespace conflict). 

First we define the web address into `vic_test_url` and use `polite`'s `bow` function to check we are allowed to scrape the data:

```{r bow}
vic_test_url <- "https://covidlive.com.au/report/daily-positive-test-rate/vic"

bow(vic_test_url) 
```

OK, looks like we're all set to go, let's `scrape` the data. This is another function from `polite` that follows the rule set from `bow` - making sure here to obey the crawl delay, and only to scrape if `bow` allows it. 

```{r bow-scrape}
bow(vic_test_url) %>% 
  scrape() 
```

A shout out to [Dmytro Perepolkin](https://github.com/dmi3kno), the creator of `polite`, such a lovely package.

This gives us this HTML document. Looking at the website, I'm fairly sure it is a nice HTML table, and we can confirm this using developer tools in Chrome (or your browser of choice)

```{r developer-tools, echo = FALSE} 
knitr::include_graphics("figs/covid-live-site.png")
```

There are many ways to extract the right part of the site, but I like to just try getting the HTML table out using `html_table()`. We're going to look at the output using `str()`, which provides a summary of the **str**ucture of the data to save ourselves printing all the HTML tables


```{r html-table}
bow(vic_test_url) %>% 
  scrape() %>% 
  html_table() %>% 
  str()
```

This tells us we want the second list element, which is the data frame, and then make that a `tibble` for nice printing:

```{r pluck-tibble}
bow(vic_test_url) %>% 
  scrape() %>% 
  html_table() %>% 
  pluck(2) %>% 
  as_tibble()
```


All together now:

```{r scrape-all-together}
vic_test_url <- "https://covidlive.com.au/report/daily-positive-test-rate/vic"

vic_test_data_raw <- bow(vic_test_url) %>% 
  scrape() %>% 
  html_table() %>% 
  purrr::pluck(2) %>% 
  as_tibble()

vic_test_data_raw
```

OK awesome, now let's format the dates. We've got them in the format of the Day of the month in decimal form and then the 3 letter month abbreviation. We can convert this into a nice date object using `strptime`. This is a function I always forget how to use, so I end up browsing the help file every time and playing with a toy example until I get what I want. There are probably better ways, but this seems to work for me.

What this says is:

```{r strptime-small}
strptime("05 Oct", format = "%d %b") 
```

- Take the string, "05 Oct"
- The format that this follows is
  - Day of the month as decimal number (01â€“31) (represented as "%d")
  - followed by a space, then 
  - Abbreviated month name in the current locale on this platform. (Also matches full name on input: in some locales there are no abbreviations of names.) (represented as "%d").
  
For this to work, we need the string in the `format` argument to match EXACTLY the input. For example:
  
```{r strptime-dash-fail}
strptime("05-Oct", format = "%d %b") 
```

Doesn't work (because the dash)

But this:

```{r strptime-dash-win}
strptime("05-Oct", format = "%d-%b") 
```

Does work, because the dash is in the `format` srtring.

OK and we want that as a `Date` object:

```{r strptime-example-date}
strptime("05 Oct", format = "%d %b") %>% as.Date()
```

Let's wrap this in a little function we can use on our data:

```{r strp-date-fun}
strp_date <- function(x) as.Date(strptime(x, format = "%d %b"))
```

And double check it works:

```{r strp-date-check}
strp_date("05 Oct")
strp_date("05 Oct") %>% class()
```

Ugh, dates.

OK, so now let's clean up the dates.

```{r clean-dates}
vic_test_data_raw %>% 
  mutate(DATE = strp_date(DATE))
```

And let's use `parse_number` to convert `TESTS` and `POS` into numbers, as they have commas in them and % signs, so R registers them as character strings. 

```{r parse-number}
vic_test_data_raw %>% 
  mutate(DATE = strp_date(DATE),
         TESTS = parse_number(TESTS),
         POS = parse_number(POS))
```

`parse_number()` (from [`readr`](https://readr.tidyverse.org/)) is one of my favourite little functions, as this saves me a ton of effort.

Now let's use `clean_names()` function from `janitor` to make the names all lower case, making them a bit nicer to deal with. (I don't like holding down shift to type all caps for long periods of time, unless I've got something exciting to celebrate or scream).

```{r clean-names}
vic_test_data_raw %>% 
  mutate(DATE = strp_date(DATE),
         TESTS = parse_number(TESTS),
         POS = parse_number(POS)) %>% 
  janitor::clean_names() 
```

And then finally all together now, I'm going to turn this into a [`tsibble`](https://tsibble.tidyverts.org/) - a time series `tibble`, using `as_tsibble`, and specifying the `index` (the time part) as the `date` column. I use this because later on we'll be manipulating the date column, and `tsibble` makes this much easier.

```{r all-together}
library(tsibble)
vic_tests <- vic_test_data_raw %>% 
  mutate(DATE = strp_date(DATE),
         TESTS = parse_number(TESTS),
         POS = parse_number(POS)) %>% 
  janitor::clean_names() %>% 
  rename(pos_pct = pos) %>% 
  as_tsibble(index = date)
```

OK, now to iterate on a few plots.

```{r vic-tests-m1}
ggplot(vic_tests,
         aes(x = date,
             y = pos_pct)) + 
  geom_line() 
```

Oof, OK, let's remove that negative date, not sure why that is there:

```{r filter-plot}
vic_tests_clean <- vic_tests %>% 
  filter(pos_pct >= 0)

ggplot(vic_tests_clean,
         aes(x = date,
             y = pos_pct)) + 
  geom_line() 
```

OK, looks like in April we have some high numbers, let's bring filter out those dates from before May using `filter_index` - here we specify the start date, and the `.` means the last date:

```{r filter-index}
vic_tests_clean %>% 
  filter_index("2020-05-01" ~ .) %>% 
  ggplot(aes(x = date,
             y = pos_pct)) + 
  geom_line() 
```

OK, much nicer. Looks like things are on the downward-ish. But the I want to add "%" signs to the plot. We could glue/paste those onto the data values, but I prefer to use the [`scales`](https://scales.r-lib.org/) package for this part. We can browse the [`label_percent()`](https://scales.r-lib.org/reference/label_percent.html) reference page to see how to use it:

```{r scales-label}
library(scales)
vic_tests_clean %>% 
  filter_index("2020-05-01" ~ .) %>% 
  ggplot(aes(x = date,
             y = pos_pct)) + 
  geom_line() +
  scale_y_continuous(labels = label_percent())

```

We specify how we want to change the y axis, using `scale_y_continuous`, and then say that the labels on the y axis need to have the `label_percent` function applied to them. Well, that's how I read it.

OK, but this isn't quite what we want actually, we need to change the scale - since by default it multiplies the number by 100. We also need to change the accuracy, since we want this to 2 decimal places. We can see this with the `percent` function, which is what `label_percent` uses under the hood.

```{r percent-example}
percent(0.1)
percent(0.1, scale = 1)
percent(0.1, scale = 1, accuracy = 0.01)
```


So now we change the `accuracy` and `scale` arguments so we get the right looking marks. 

```{r scales-final}
library(scales)
vic_tests_clean %>% 
  filter_index("2020-05-01" ~ .) %>% 
  ggplot(aes(x = date,
             y = pos_pct)) + 
  geom_line() +
  scale_y_continuous(labels = label_percent(accuracy = 0.01, 
                                            scale = 1))

```

And that's how to scrape some data, parse the dates, filter by time,  and make the percentages print nice in a ggplot.

Thanks to [Dmytro Perepolkin](https://github.com/dmi3kno) for `polite`, [Earo Wang](https://earo.me/) for `tsibble`, [Sam Firke](http://samfirke.com/about/) for `janitor`, the awesome [`tidyverse`](https://www.tidyverse.org/) team for creating and maintaining the `tidyverse`, and of course the folks behind R, because R is great.
